# Continual Learning: Adapting Prompts Intelligently for Shifting Production Traffic

*How MIPRO online and warm-started GEPA outperform cold-start optimization when your data distribution shifts*

**Tags:** continual-learning, mipro, gepa, prompt-optimization, distribution-shift

**Read time:** 3 min

---

## The Problem: Your Data Distribution Has Changed, Now What?

You've optimized your prompt with GEPA. It's great — until you start targeting a new user segment or your power users start utilizing the product in a new way.

What do you do — throw away old data/prompts and start from scratch with a new GEPA run?

One option is to start from scratch — **cold starting** — and the other is to train on new data starting with the old-best prompt — **warm starting**. Both have tradeoffs:
- **Cold start** throws away everything you learned
- **Warm start** prior prompt structure might over-constrain on the new distribution

We also test **MIPRO online** with ontology support, which updates incrementally on streamed data, evolves an ontology of learned observations, and retains successful patterns while adapting to new ones — all without restarts.

## Benchmark: Banking77 Progressive Splits

We tested on Banking77, a real-world intent classification dataset with 77 banking intents. We created 4 progressive data splits, each a superset of the previous:

| Split | Intents | Example Categories |
|-------|---------|-------------------|
| Split 1 | 2 | Card arrival, lost card |
| Split 2 | 7 | + Card payments, PIN, activation |
| Split 3 | 27 | + Transfers, fees, account issues |
| Split 4 | 77 | All banking intents |

This simulates a realistic scenario: you start with a focused use case, then expand as your product grows.

### Held-Out Test Results

All methods trained with 100 rollouts per split using `gpt-4.1-nano`, evaluated on the **full held-out test set** (80 to 3,080 samples per split).

| Split | Intents | Test N | Baseline | MIPRO | GEPA Cold | GEPA Warm | Best Method |
|-------|---------|--------|----------|-------|-----------|-----------|-------------|
| Split 1 | 2 | 80 | 97.5% | **97.5%** | 96.2% | — | Baseline / MIPRO |
| Split 2 | 7 | 280 | 85.7% | **87.1%** | 86.1% | 83.6% | **MIPRO** |
| Split 3 | 27 | 1080 | 60.9% | 66.6% | 61.4% | **66.7%** | **GEPA Warm / MIPRO** |
| Split 4 | 77 | 3080 | 54.6% | 56.0% | **57.6%** | **57.6%** | **GEPA Cold / Warm** |

### Key Findings

**MIPRO online beats baseline on every split.** With ontology-driven continual learning, MIPRO adapts its prompt as the data distribution shifts — no restarts needed. It wins outright on Split 2 (+1.4pp) and ties GEPA Warm on Split 3 (+5.7pp over baseline).

**GEPA Warm and MIPRO converge on the hardest splits.** On Split 3 (27 intents), GEPA Warm and MIPRO are essentially tied at ~66.6-66.7%. Both outperform cold-start GEPA and baseline by 5+ percentage points, confirming that knowledge retention matters when task complexity grows.

**GEPA Cold/Warm edge out on Split 4.** At full 77-intent complexity, GEPA methods (57.6%) lead MIPRO (56.0%) by 1.6pp. With 100 rollouts per split, GEPA has more optimization budget focused on each distribution, while MIPRO spreads its budget across continual adaptation.

### The Crossover Point

On simpler tasks (2 intents), all methods match baseline. Starting at 7 intents, MIPRO's continual adaptation gives it an edge. At 27 intents, both MIPRO and GEPA Warm deliver ~6pp gains over baseline. The crossover happens where task complexity benefits from retained disambiguation patterns — and both warm-start approaches capture this.

## When to Use Each Approach

| Scenario | Recommendation |
|----------|----------------|
| One-time optimization on stable data | **GEPA Cold** |
| Small, focused task (<10 categories) | **GEPA Cold** |
| Task complexity grows over time | **GEPA Warm** or **MIPRO Online** |
| Need to adapt without restarts | **MIPRO Online** |
| Continuous production traffic | **MIPRO Online** |

## Try It Yourself

The full demo is available in our SDK:

```bash
cd demos/continual_learning_banking77

# Run the full comparison with held-out eval
uv run python run_held_out_eval.py --eval-only \
    --mipro-results results/mipro_continual_20260131_002253.json \
    --gepa-results results/classic_gepa_20260130_184037.json

# Or run fresh training + eval
uv run python run_held_out_eval.py --rollouts-per-split 100
```

## How MIPRO Online Works

MIPRO online maintains a graph ontology that accumulates learned knowledge — disambiguation patterns, scoring strategies, and failure modes — as it processes streamed production data. Each new candidate prompt is generated by a proposer that incorporates this ontology context, producing atomic instruction transforms (individual rules, mappings, and constraints) that build on each other across distribution shifts.

Key to making this work: a windowed best-candidate selection that only considers recently-tested candidates. Without this, high-scoring prompts from earlier (easier) distributions would dominate forever, preventing adaptation to harder tasks.

---

*Ready to try continual prompt learning? [Get started with Synth AI](https://www.usesynth.ai) or check out the [documentation](https://docs.usesynth.ai).*
